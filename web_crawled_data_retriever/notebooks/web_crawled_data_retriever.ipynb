{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web crawling RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p4/y0q2kh796nx_k_yzfhxs57f00000gp/T/ipykernel_79160/3922346559.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, '..'))\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "from tqdm.autonotebook import trange\n",
    "import nest_asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, urldefrag\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import AsyncHtmlLoader\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain_community.llms.sambanova import SambaStudio\n",
    "from langchain_community.document_loaders import UnstructuredURLLoader\n",
    "\n",
    "from utils.model_wrappers.langchain_llms import SambaNovaCloud\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_remote_pdf(url):\n",
    "    \"\"\"\n",
    "    Load PDF files from the given URL.\n",
    "    Args:\n",
    "        url (str): URL to load pdf document from.\n",
    "    Returns:\n",
    "        list: A list of loaded pdf documents.\n",
    "    \"\"\"\n",
    "    loader = UnstructuredURLLoader(urls=[url])\n",
    "    docs = loader.load()\n",
    "    return docs\n",
    "\n",
    "\n",
    "def load_htmls(urls, extra_loaders=None):\n",
    "    \"\"\"\n",
    "    Load HTML documents from the given URLs.\n",
    "    Args:\n",
    "        urls (list): A list of URLs to load HTML documents from.\n",
    "    Returns:\n",
    "        list: A list of loaded HTML documents.\n",
    "    \"\"\"\n",
    "    if extra_loaders is None:\n",
    "        extra_loaders = []\n",
    "    docs = []\n",
    "    for url in urls:\n",
    "        if url.endswith('.pdf'):\n",
    "            if 'pdf' in extra_loaders:\n",
    "                docs.extend(load_remote_pdf(url))\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            loader = AsyncHtmlLoader(url, verify_ssl=False)\n",
    "            docs.extend(loader.load())\n",
    "    return docs\n",
    "\n",
    "\n",
    "def link_filter(all_links, excluded_links):\n",
    "    \"\"\"\n",
    "    Filters a list of links based on a list of excluded links.\n",
    "    Args:\n",
    "        all_links (List[str]): A list of links to filter.\n",
    "        excluded_links (List[str]): A list of excluded links.\n",
    "    Returns:\n",
    "        Set[str]: A list of filtered links.\n",
    "    \"\"\"\n",
    "    clean_excluded_links = set()\n",
    "    for excluded_link in excluded_links:\n",
    "        parsed_link = urlparse(excluded_link)\n",
    "        clean_excluded_links.add(parsed_link.netloc + parsed_link.path)\n",
    "    filtered_links = set()\n",
    "    for link in all_links:\n",
    "        # Check if the link contains any of the excluded links\n",
    "        if not any(excluded_link in link for excluded_link in clean_excluded_links):\n",
    "            filtered_links.add(link)\n",
    "    return filtered_links\n",
    "\n",
    "\n",
    "def find_links(docs, excluded_links=None):\n",
    "    \"\"\"\n",
    "    Find links in the given HTML documents, excluding specified links and not text content links.\n",
    "    Args:\n",
    "        docs (list): A list of documents with html content to search for links.\n",
    "        excluded_links (list, optional): A list of links to exclude from the search. Defaults to None.\n",
    "    Returns:\n",
    "        set: A set of unique links found in the HTML documents.\n",
    "    \"\"\"\n",
    "    if excluded_links is None:\n",
    "        excluded_links = []\n",
    "    all_links = set()\n",
    "    excluded_link_suffixes = {'.ico', '.svg', '.jpg', '.png', '.jpeg', '.', '.docx', '.xls', '.xlsx'}\n",
    "    for doc in docs:\n",
    "        page_content = doc.page_content\n",
    "        base_url = doc.metadata['source']\n",
    "        # excluded_links.append(base_url)\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "        # Identify the main content section (customize based on HTML structure)\n",
    "        main_content = soup.find('main') or soup.find('article') or soup.find('div', class_='content')\n",
    "        if main_content:\n",
    "            links = main_content.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                # Check if the link is not an anchor link and not in the excluded links or suffixes\n",
    "                if not href.startswith(('#', 'data:', 'javascript:')) and not any(\n",
    "                    href.endswith(suffix) for suffix in excluded_link_suffixes\n",
    "                ):\n",
    "                    full_url, _ = urldefrag(urljoin(base_url, href))\n",
    "                    all_links.add(full_url)\n",
    "    all_links = link_filter(all_links, set(excluded_links))\n",
    "    return all_links\n",
    "\n",
    "\n",
    "def clean_docs(docs):\n",
    "    \"\"\"\n",
    "    Clean the given HTML documents by transforming them into plain text.\n",
    "    Args:\n",
    "        docs (list): A list of langchain documents with html content to clean.\n",
    "    Returns:\n",
    "        list: A list of cleaned plain text documents.\n",
    "    \"\"\"\n",
    "    html2text_transformer = Html2TextTransformer()\n",
    "    docs = html2text_transformer.transform_documents(documents=docs)\n",
    "    return docs\n",
    "\n",
    "\n",
    "def web_crawl(urls, excluded_links=None, depth=1):\n",
    "    \"\"\"\n",
    "    Perform web crawling, retrieve and clean HTML documents from the given URLs, with specified depth of exploration.\n",
    "    Args:\n",
    "        urls (list): A list of URLs to crawl.\n",
    "        excluded_links (list, optional): A list of links to exclude from crawling. Defaults to None.\n",
    "        depth (int, optional): The depth of crawling, determining how many layers of internal links to explore. Defaults to 1\n",
    "    Returns:\n",
    "        tuple: A tuple containing the langchain documents (list) and the scrapped URLs (list).\n",
    "    \"\"\"\n",
    "    if excluded_links is None:\n",
    "        excluded_links = []\n",
    "    if depth > 3:\n",
    "        depth = 3\n",
    "    scrapped_urls = []\n",
    "    raw_docs = []\n",
    "    for _ in range(depth):\n",
    "        scraped_docs = load_htmls(urls, extra_loaders=['pdf'])\n",
    "        scrapped_urls.extend(urls)\n",
    "        urls = find_links(scraped_docs, excluded_links)\n",
    "        excluded_links.extend(scrapped_urls)\n",
    "        raw_docs.extend(scraped_docs)\n",
    "    docs = clean_docs(scraped_docs)\n",
    "    return docs, scrapped_urls\n",
    "\n",
    "\n",
    "def get_text_chunks(docs):\n",
    "    \"\"\"\n",
    "    Split the given docuemnts into smaller chunks.\n",
    "    Args:\n",
    "        docs (list): The documents to be split into chunks.\n",
    "    Returns:\n",
    "        list: A list of documents with text chunks.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, length_function=len)\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def get_vectorstore(text_chunks):\n",
    "    \"\"\"\n",
    "    Create and return a Vector Store for a collection of text chunks.\n",
    "    This function generates a vector store using the FAISS library, which allows efficient similarity search\n",
    "    over a collection of text chunks by representing them as embeddings.\n",
    "    Args:\n",
    "        text_chunks (list of str): A list of text chunks or sentences to be stored and indexed for similarity search.\n",
    "    Returns:\n",
    "        FAISSVectorStore: A Vector Store containing the embeddings of the input text chunks, suitable for similarity search operations.\n",
    "    \"\"\"\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    embeddings = HuggingFaceInstructEmbeddings(\n",
    "        model_name='BAAI/bge-large-en',\n",
    "        embed_instruction='',  # no instruction is needed for candidate passages\n",
    "        query_instruction='Represent this paragraph for searching relevant passages: ',\n",
    "        encode_kwargs=encode_kwargs,\n",
    "    )\n",
    "    vectorstore = FAISS.from_documents(documents=text_chunks, embedding=embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "def get_custom_prompt():\n",
    "    \"\"\"\n",
    "    Generate a custom prompt template for contextual question answering.\n",
    "    This function creates and returns a custom prompt template that instructs the model on how to answer a question\n",
    "    based on the provided context. The template includes placeholders for the context and question to be filled in\n",
    "    when generating prompts.\n",
    "    Returns:\n",
    "        PromptTemplate: A custom prompt template for contextual question answering.\n",
    "    \"\"\"\n",
    "    custom_prompt_template = \"\"\"<s>[INST] <<SYS>>\\n\"Use the following pieces of context to answer the question at the end. \n",
    "        If the answer is not in context for answering, say that you don't know, don't try to make up an answer or provide an answer not extracted from provided context. \n",
    "        Cross check if the answer is contained in provided context. If not than say \"I do not have information regarding this.\" \n",
    "\n",
    "        context\n",
    "        {context}\n",
    "        end of context\n",
    "        <</SYS>>\n",
    "\n",
    "        Question: {question}\n",
    "        Helpful Answer: [/INST]\"\"\"\n",
    "\n",
    "    CUSTOMPROMPT = PromptTemplate(template=custom_prompt_template, input_variables=['context', 'question'])\n",
    "    return CUSTOMPROMPT\n",
    "\n",
    "\n",
    "def get_retriever_qa(vectorstore):\n",
    "    \"\"\"\n",
    "    Generate a qa_retrieval chain using a language model.\n",
    "    This function uses a language model, specifically a SambaNovaEndpoint, to generate a qa_retrieval chain\n",
    "    based on the input vector store of text chunks.\n",
    "    Args:\n",
    "        vectorstore (FAISSVectorStore): A Vector Store containing embeddings of text chunks used as context\n",
    "                                    for generating the conversation chain.\n",
    "    Returns:\n",
    "        RetrievalQA: A chain ready for QA without memory\n",
    "    \"\"\"\n",
    "\n",
    "    # SambaNova Cloud LLM\n",
    "    llm = SambaNovaCloud(\n",
    "        max_tokens=1200,\n",
    "        model='llama3-70b',\n",
    "    )\n",
    "\n",
    "    # SambaStudio LLM\n",
    "    # llm = SambaStudio(\n",
    "    #    model_kwargs={\n",
    "    #        'max_tokens': 1200,\n",
    "    #        'model': 'Meta-Llama-3-70B-Instruct',\n",
    "    #        'process_prompt': False,\n",
    "    #    },\n",
    "    # )\n",
    "\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type='similarity_score_threshold',\n",
    "        search_kwargs={'score_threshold': 0.5, 'k': 4},\n",
    "    )\n",
    "    retrieval_chain = RetrievalQA.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        input_key='question',\n",
    "        output_key='answer',\n",
    "    )\n",
    "    ## Inject custom prompt\n",
    "    retrieval_chain.combine_documents_chain.llm_chain.prompt = get_custom_prompt()\n",
    "    return retrieval_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 1/1 [00:01<00:00,  1.04s/it]\n",
      "Fetching pages: 100%|##########| 1/1 [00:00<00:00,  3.06it/s]\n",
      "Fetching pages: 100%|##########| 1/1 [00:01<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "filtered_sites = [\n",
    "    'facebook.com',\n",
    "    'twitter.com',\n",
    "    'instagram.com',\n",
    "    'linkedin.com',\n",
    "    'telagram.me',\n",
    "    'reddit.com',\n",
    "    'whatsapp.com',\n",
    "    'wa.me',\n",
    "]\n",
    "urls = ['https://www.espn.com', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://sambanova.ai/']\n",
    "docs, urls = web_crawl(urls, excluded_links=filtered_sites, depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.espn.com',\n",
       " 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'https://sambanova.ai/']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n"
     ]
    }
   ],
   "source": [
    "text_chunks = get_text_chunks(docs)\n",
    "print(len(text_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vector store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "vectorstore = get_vectorstore(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the language model, and the retrievalQA chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(os.path.join(repo_dir, '.env'))\n",
    "retrieval_chain = get_retriever_qa(vectorstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_question = \"which are the mars expeditions?\"\n",
    "# user_question = \"what it means planning in an llm agent\"\n",
    "# user_question = \"wich are the games for today?\"\n",
    "# user_question = \"what is the SN40?\"\n",
    "user_question = 'which kinds of memory can an agent have?'\n",
    "response = retrieval_chain.invoke({'question': user_question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response =According to the provided context, an agent can have the following kinds of memory:\n",
      "\n",
      "1. **Short-term memory**: This is equivalent to in-context learning, which is short and finite, restricted by the finite context window length of the Transformer.\n",
      "2. **Long-term memory**: This is an external vector store that the agent can attend to at query time, accessible via fast retrieval.\n",
      "\n",
      "Additionally, the context also mentions **Sensory memory**, which is equivalent to learning embedding representations for raw inputs, including text, image, or other modalities. However, this is not explicitly stated as a type of memory that an agent can have, but rather as a rough mapping to human memory.\n"
     ]
    }
   ],
   "source": [
    "print(f'Response ={response[\"answer\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
