{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SambanNova Langchain Wrappers Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jorgep/Documents/ask_public_own/testenv/lib/python3.11/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_kwargs\" in SambaStudioEmbeddings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_embeddings import SambaStudioEmbeddings\n",
    "from langchain_llms import SambaStudio, SambaNovaCloud\n",
    "from langchain_chat_models import ChatSambaNovaCloud, ChatSambaStudio\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "utils_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "repo_dir = os.path.abspath(os.path.join(utils_dir, '..'))\n",
    "\n",
    "load_dotenv(os.path.join(repo_dir, '.env'), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SambaStudio LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = SambaStudio(\n",
    "    model_kwargs={\n",
    "        'do_sample': False,\n",
    "        'temperature': 0.01,\n",
    "        'max_tokens': 256,\n",
    "        'process_prompt': False,\n",
    "        'model': 'Meta-Llama-3-70B-Instruct-4096',\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As the clock struck midnight, a lone figure crept into the abandoned mansion. Suddenly, a chandelier crashed down, and a ghostly figure materialized. \"You shouldn\\'t have come here,\" it whispered. The intruder froze, trapped in a century-old curse, forever doomed to roam the haunted halls.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('tell me a 50 word tale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = SambaStudio(\n",
    "    model_kwargs={\n",
    "        'do_sample': False,\n",
    "        'max_tokens': 256,\n",
    "        'temperature': 0.01,\n",
    "        'process_prompt': False,\n",
    "        'model': 'Meta-Llama-3-70B-Instruct-4096',\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As the clock struck midnight, a lone figure crept into the abandoned mansion. Suddenly, a chandelier crashed down, and a ghostly figure materialized. \"You shouldn't have come here,\" it whispered. The intruder froze, trapped in a century-old curse, forever doomed to roam the haunted halls."
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream('tell me a 50 word tale'):\n",
    "    print(chunk, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SambaNovaCloud LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = SambaNovaCloud(model='llama3-70b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello. How can I assist you today?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "llm.invoke(json.dumps([{'role': 'user', 'content': 'hello'}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello. How can I assist you today?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here's a long story \n",
      "for you:\n",
      "\n",
      "Once upon \n",
      "a time, in a small village \n",
      "nestled in the rolling hills of \n",
      "rural France, there lived a \n",
      "young girl named Sophie. Sophie \n",
      "was a curious and adventurous \n",
      "child, with a mop of curly \n",
      "brown hair and a smile that \n",
      "could light up the darkest \n",
      "of rooms. She lived with \n",
      "her parents, Pierre and \n",
      "Colette, in a small stone cottage \n",
      "on the outskirts of \n",
      "the village.\n",
      "\n",
      "Sophie's village was \n",
      "a charming \n",
      "place, filled with narrow \n",
      "cobblestone streets, quaint shops, \n",
      "and \n",
      "bustling cafes. The villagers \n",
      "were a tight-knit \n",
      "community, and everyone knew each \n",
      "other's names and stories. Sophie \n",
      "loved listening to the villagers' \n",
      "tales of \n",
      "old, which \n",
      "often featured brave knights, \n",
      "beautiful princesses, and \n",
      "magical creatures.\n",
      "\n",
      "One day, while exploring \n",
      "the village, Sophie stumbled upon \n",
      "a small, mysterious shop tucked \n",
      "away on a quiet street. \n",
      "The sign above the door \n",
      "read \"Curios \n",
      "and Wonders,\" and the \n",
      "windows were filled \n",
      "with a dazzling array of strange \n",
      "and exotic objects. Sophie's \n",
      "curiosity was piqued, \n",
      "and she pushed open the door \n",
      "to venture inside.\n",
      "\n",
      "The shop \n",
      "was dimly lit, and \n",
      "the air was thick with the \n",
      "scent of old books and \n",
      "dust. Sophie's eyes \n",
      "adjusted slowly, and she \n",
      "saw that the shop was filled \n",
      "with all manner of curious \n",
      "objects: vintage \n",
      "clocks, rare coins, \n",
      "and even a \n",
      "taxidermied owl perched on \n",
      "a shelf. Behind the counter stood \n",
      "an old man with a kind \n",
      "face \n",
      "and a twinkle in his eye.\n",
      "\n",
      "\n",
      "\n",
      "\"Bonjour, mademoiselle,\" he \n",
      "said, his voice low and \n",
      "soothing. \"Welcome to Curios \n",
      "and Wonders. I \n",
      "am Monsieur LaFleur, \n",
      "the proprietor. How may I \n",
      "assist you \n",
      "today?\"\n",
      "\n",
      "Sophie wandered the aisles, \n",
      "running her fingers over \n",
      "the strange objects on \n",
      "display. She picked up \n",
      "a small, delicate music \n",
      "box and wound \n",
      "it up, listening \n",
      "as it played \n",
      "a soft, melancholy \n",
      "tune. Monsieur LaFleur \n",
      "smiled and nodded \n",
      "in approval.\n",
      "\n",
      "\"Ah, you have a \n",
      "good ear for \n",
      "music, mademoiselle,\" he \n",
      "said. \"That music box \n",
      "is a \n",
      "rare and precious item. It \n",
      "was crafted by a skilled artisan \n",
      "in the 18th century.\"\n",
      "\n",
      "\n",
      "As Sophie continued to \n",
      "explore the shop, \n",
      "she stumbled upon \n",
      "a large, leather-bound book \n",
      "with strange symbols etched into \n",
      "the cover. \n",
      "Monsieur LaFleur noticed her interest and \n",
      "approached \n",
      "her.\n",
      "\n",
      "\"Ah, you've found \n",
      "the infamous 'Livre \n",
      "\n",
      "des Secrets,'\" \n",
      "he said, his \n",
      "voice low and mysterious. \n",
      "\"That book is said to contain \n",
      "the secrets of the universe, \n",
      "hidden within its pages. But \n",
      "be \n",
      "warned, mademoiselle, \n",
      "the book is said to \n",
      "be cursed. Many have attempted \n",
      "to unlock its secrets, but \n",
      "none have \n",
      "succeeded.\"\n",
      "\n",
      "Sophie's eyes widened with \n",
      "excitement as she carefully opened \n",
      "the book. The pages \n",
      "were yellowed and \n",
      "crackling, and \n",
      "the text was written in a \n",
      "language she couldn't understand. \n",
      "But as she turned the \n",
      "pages, \n",
      "she felt a strange sensation, \n",
      "as if the book \n",
      "was calling \n",
      "to her.\n",
      "\n",
      "Monsieur \n",
      "LaFleur smiled \n",
      "and \n",
      "nodded. \"I see you have a \n",
      "connection to the \n",
      "book, mademoiselle. Perhaps you \n",
      "are the one who can unlock \n",
      "its secrets.\"\n",
      "\n",
      "Over the next \n",
      "few weeks, Sophie returned to \n",
      "the shop again and again, \n",
      "pouring over \n",
      "the pages of the Livre \n",
      "des Secrets. She spent hours \n",
      "studying \n",
      "the symbols and trying to decipher \n",
      "the text. \n",
      "Monsieur \n",
      "LaFleur watched her with a \n",
      "keen eye, offering guidance and encouragement \n",
      "whenever she needed it.\n",
      "\n",
      "As \n",
      "the days turned into weeks, \n",
      "Sophie began to notice strange occurrences \n",
      "happening around her. She would \n",
      "find objects moved from their \n",
      "usual places, and she would hear \n",
      "whispers in the night. She \n",
      "began \n",
      "to feel as though the book \n",
      "was exerting some kind of \n",
      "influence over her, drawing her \n",
      "deeper into \n",
      "its secrets.\n",
      "\n",
      "One \n",
      "night, Sophie had a vivid dream \n",
      "in which \n",
      "she saw herself standing in \n",
      "a \n",
      "grand, \n",
      "candlelit hall. \n",
      "The walls were lined with \n",
      "ancient tapestries, and the \n",
      "air was thick with the scent \n",
      "of \n",
      "incense. At the far end of \n",
      "the hall, she saw a \n",
      "figure cloaked in shadows.\n",
      "\n",
      "\n",
      "As she approached \n",
      "the figure, it stepped forward, \n",
      "revealing a woman \n",
      "with long, flowing hair and \n",
      "piercing green eyes. The woman \n",
      "spoke in a voice that was \n",
      "both familiar and yet \n",
      "completely alien.\n",
      "\n",
      "\"Sophie, you \n",
      "have been chosen to unlock the \n",
      "secrets of the Livre \n",
      "des Secrets,\" she \n",
      "said. \"But be warned, \n",
      "the \n",
      "journey will \n",
      "be difficult, and the cost \n",
      "will be high. Are you \n",
      "prepared to pay \n",
      "the price?\"\n",
      "\n",
      "Sophie woke up with \n",
      "a start, her heart racing and \n",
      "her mind reeling. She \n",
      "knew that she had \n",
      "to return to the shop and \n",
      "confront Monsieur LaFleur \n",
      "about the \n",
      "strange \n",
      "occurrences. But when she \n",
      "arrived at the shop, she \n",
      "found that it \n",
      "was closed, \n",
      "and \n",
      "a sign on the door \n",
      "read \"Gone on \n",
      "a \n",
      "journey. Will return \n",
      "soon.\"\n",
      "\n",
      "Sophie \n",
      "was devastated. \n",
      "She felt as though she had \n",
      "been abandoned, left \n",
      "to navigate the mysteries of \n",
      "the Livre des Secrets on \n",
      "her own. But as \n",
      "she turned to leave, she \n",
      "noticed a\n"
     ]
    }
   ],
   "source": [
    "for i in llm.stream('hello tell me a long story'):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SambaStudio Chat Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatSambaStudio(\n",
    "    model=\"Meta-Llama-3-70B-Instruct-4096\",\n",
    "    max_tokens=1024,\n",
    "    temperature=0.3,\n",
    "    top_k=1,\n",
    "    top_p=0.01,\n",
    "    do_sample = True,\n",
    "    process_prompt = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Here's one:\\n\\nWhy couldn't the bicycle stand up by itself?\\n\\n(Wait for it...)\\n\\nBecause it was two-tired!\\n\\nHope that made you laugh!\", additional_kwargs={}, response_metadata={'finish_reason': None, 'usage': {'prompt_tokens': 14, 'completion_tokens': 32, 'total_tokens': 46, 'throughput_after_first_token': 70.33306137927788, 'time_to_first_token': 0.22188520431518555, 'model_execution_time': 0.5915548801422119}, 'model_name': 'Meta-Llama-3-70B-Instruct-4096', 'system_fingerprint': '', 'created': 1727913501}, id='126f946f-3cbf-4739-b00b-bf99fa864d48')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Arrr, listen up, matey! Here be a joke fer ye:\\n\\nWhy did the pirate quit his job?\\n\\nBecause he was sick o\\' all the arrrr-guments! (get it? arguments, but with a pirate \"arrr\" sound? Aye, I be a regular comedic genius, savvy?)\\n\\nSo, did I make ye laugh, or did I walk the plank?', additional_kwargs={}, response_metadata={'finish_reason': None, 'usage': {'prompt_tokens': 27, 'completion_tokens': 80, 'total_tokens': 107, 'throughput_after_first_token': 71.2460325330454, 'time_to_first_token': 0.21974945068359375, 'model_execution_time': 1.2584037780761719}, 'model_name': 'Meta-Llama-3-70B-Instruct-4096', 'system_fingerprint': '', 'created': 1727913503}, id='1f48b1e1-19cb-43a2-852d-c4b11a8ab3b4')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant with pirate accent\"),\n",
    "    HumanMessage(content=\"tell me a joke\"),\n",
    "]\n",
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Here's one:\\n\\nWhy couldn't the bicycle stand up by itself?\\n\\n(Wait for it...)\\n\\nBecause it was two-tired!\\n\\nHope that made you laugh!\", additional_kwargs={}, response_metadata={'finish_reason': None, 'usage': {'prompt_tokens': 14, 'completion_tokens': 32, 'total_tokens': 46, 'throughput_after_first_token': 70.43063268918901, 'time_to_first_token': 0.22060894966125488, 'model_execution_time': 0.5897665023803711}, 'model_name': 'Meta-Llama-3-70B-Instruct-4096', 'system_fingerprint': '', 'created': 1727913507}, id='5960e729-b41d-4631-ac90-af1e8188b654')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_response = llm.ainvoke(\"tell me a joke\")\n",
    "await future_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=\"Here's one:\\n\\nWhy couldn't the bicycle stand up by itself?\\n\\n(Wait for it...)\\n\\nBecause it was two-tired!\\n\\nHope that made you laugh!\", additional_kwargs={}, response_metadata={'finish_reason': None, 'usage': {'prompt_tokens': 14, 'completion_tokens': 32, 'total_tokens': 46, 'throughput_after_first_token': 70.06468839908355, 'time_to_first_token': 0.2191941738128662, 'model_execution_time': 0.5902798175811768}, 'model_name': 'Meta-Llama-3-70B-Instruct-4096', 'system_fingerprint': '', 'created': 1727913510}, id='f6ef319e-4ba2-4117-8d63-f5823d0bc947'),\n",
       " AIMessage(content='The capital of the United Kingdom (UK) is London.', additional_kwargs={}, response_metadata={'finish_reason': None, 'usage': {'prompt_tokens': 17, 'completion_tokens': 12, 'total_tokens': 29, 'throughput_after_first_token': 62.08816650383397, 'time_to_first_token': 0.21888446807861328, 'model_execution_time': 0.315521240234375}, 'model_name': 'Meta-Llama-3-70B-Instruct-4096', 'system_fingerprint': '', 'created': 1727913509}, id='6145cc9d-7db4-4699-a47a-22445a7fdc49')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.batch([\"tell me a joke\", \"which is the capital of UK?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=\"Here's one:\\n\\nWhy couldn't the bicycle stand up by itself?\\n\\n(Wait for it...)\\n\\nBecause it was two-tired!\\n\\nHope that made you laugh!\", additional_kwargs={}, response_metadata={'finish_reason': None, 'usage': {'prompt_tokens': 14, 'completion_tokens': 32, 'total_tokens': 46, 'throughput_after_first_token': 70.03251686884015, 'time_to_first_token': 0.21899962425231934, 'model_execution_time': 0.5902557373046875}, 'model_name': 'Meta-Llama-3-70B-Instruct-4096', 'system_fingerprint': '', 'created': 1727913513}, id='33123501-e370-4a70-bda1-92c8af70865b'),\n",
       " AIMessage(content='The capital of the United Kingdom (UK) is London.', additional_kwargs={}, response_metadata={'finish_reason': None, 'usage': {'prompt_tokens': 17, 'completion_tokens': 12, 'total_tokens': 29, 'throughput_after_first_token': 62.36218692927395, 'time_to_first_token': 0.21871089935302734, 'model_execution_time': 0.3149230480194092}, 'model_name': 'Meta-Llama-3-70B-Instruct-4096', 'system_fingerprint': '', 'created': 1727913512}, id='d4d5e44a-c6eb-4298-be34-57b08e691931')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_responses = llm.abatch([\"tell me a joke\", \"which is the capital of UK?\"])\n",
    "await future_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "(Wait for it...)\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "Hope that made you laugh!"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(\"tell me a joke\"):\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, \n",
      "listen up, matey! Here \n",
      "be a joke fer ye:\n",
      "\n",
      "\n",
      "Why did the pirate quit his job?\n",
      "\n",
      "\n",
      "Because he was \n",
      "sick o' all \n",
      "the arrrr-guments! (get \n",
      "it? arguments, but with a \n",
      "pirate \"arrr\" \n",
      "sound? Aye, I be \n",
      "a regular comedic genius, savvy?)\n",
      "\n",
      "\n",
      "So, did I make \n",
      "ye laugh, or did I \n",
      "walk the plank?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant with pirate accent\"),\n",
    "    HumanMessage(content=\"tell me a joke\"),\n",
    "]\n",
    "for chunk in llm.stream(messages):\n",
    "    print(chunk.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "\n",
      "Why couldn't the bicycle stand \n",
      "up by itself?\n",
      "\n",
      "(Wait \n",
      "for it...)\n",
      "\n",
      "Because it \n",
      "was two-tired!\n",
      "\n",
      "Hope that \n",
      "made you laugh!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async for chunk in llm.astream(\"tell me a joke\"):\n",
    "    print(chunk.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SambaNova Cloud Chat Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatSambaNovaCloud(\n",
    "    model= \"Meta-Llama-3.1-70B-Instruct\",\n",
    "    max_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    top_k=1,\n",
    "    top_p=0.01,\n",
    "    stream_options={'include_usage':True}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='What do you call a fake noodle?\\n\\nAn impasta.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'usage': {'acceptance_rate': 19, 'completion_tokens': 13, 'completion_tokens_after_first_per_sec': 475.9673935657141, 'completion_tokens_after_first_per_sec_first_ten': 1685.8848318172202, 'completion_tokens_per_sec': 88.77807543958723, 'end_time': 1731021299.575192, 'is_last_response': True, 'prompt_tokens': 39, 'start_time': 1731021299.4112587, 'time_to_first_token': 0.13872146606445312, 'total_latency': 0.1464325503299111, 'total_tokens': 52, 'total_tokens_per_sec': 355.1123017583489}, 'model_name': 'llama3-70b', 'system_fingerprint': 'fastcoe', 'created': 1731021299}, id='edda0334-df04-4caa-be62-35c106006e42')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Yer lookin' fer a joke, eh? Alright then, matey, here be one fer ye:\\n\\nWhy did the pirate quit his job?\\n\\n(pause fer dramatic effect, savvy?)\\n\\nBecause he was sick o' all the arrrr-guments!\\n\\nYarrr, I hope that made ye laugh, me hearty!\", additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'usage': {'acceptance_rate': 8, 'completion_tokens': 66, 'completion_tokens_after_first_per_sec': 559.2049736324459, 'completion_tokens_after_first_per_sec_first_ten': 707.3472605771866, 'completion_tokens_per_sec': 285.62908024958386, 'end_time': 1731021300.5634716, 'is_last_response': True, 'prompt_tokens': 47, 'start_time': 1731021300.3094726, 'time_to_first_token': 0.13776254653930664, 'total_latency': 0.2310689091682434, 'total_tokens': 113, 'total_tokens_per_sec': 489.03160709398446}, 'model_name': 'llama3-70b', 'system_fingerprint': 'fastcoe', 'created': 1731021300}, id='397e0f3c-6937-423b-b545-eb5e2c755abb')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant with pirate accent\"),\n",
    "    HumanMessage(content=\"tell me a joke\")\n",
    "    ]\n",
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='What do you call a fake noodle?\\n\\nAn impasta.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'usage': {'acceptance_rate': 19, 'completion_tokens': 13, 'completion_tokens_after_first_per_sec': 494.7814991398378, 'completion_tokens_after_first_per_sec_first_ten': 1686.5270464742234, 'completion_tokens_per_sec': 89.46786799670693, 'end_time': 1731021301.8407655, 'is_last_response': True, 'prompt_tokens': 39, 'start_time': 1731021301.678917, 'time_to_first_token': 0.13759541511535645, 'total_latency': 0.14530356306778758, 'total_tokens': 52, 'total_tokens_per_sec': 357.8714719868277}, 'model_name': 'llama3-70b', 'system_fingerprint': 'fastcoe', 'created': 1731021301}, id='3a18d9eb-9eb6-4d9c-9e13-f05272377614')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_response = llm.ainvoke(\"tell me a joke\")\n",
    "await(future_response) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatSambaNovaCloud(\n",
    "    model= \"Meta-Llama-3.1-70B-Instruct\",\n",
    "    streaming=False,\n",
    "    max_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    top_k=1,\n",
    "    top_p=0.01,\n",
    "    stream_options={'include_usage':True}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='A man walked into a library and asked the librarian, \"Do you have any books on Pavlov\\'s dogs and Schrödinger\\'s cat?\"\\n\\nThe librarian replied, \"It rings a bell, but I\\'m not sure if it\\'s here or not.\"', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'usage': {'acceptance_rate': 2.5555555555555554, 'completion_tokens': 53, 'completion_tokens_after_first_per_sec': 99.51652876347055, 'completion_tokens_after_first_per_sec_first_ten': 50.33958151804724, 'completion_tokens_per_sec': 30.84209227982437, 'end_time': 1731021309.2256646, 'is_last_response': True, 'prompt_tokens': 39, 'start_time': 1731021307.5072339, 'time_to_first_token': 1.1959044933319092, 'total_latency': 1.718430757522583, 'total_tokens': 92, 'total_tokens_per_sec': 53.53721678761966}, 'model_name': 'llama3-405b', 'system_fingerprint': 'fastcoe', 'created': 1731021307}, id='b78c8432-60e0-4fa1-9913-604cb02c98e6'),\n",
       " AIMessage(content='The capital of the United Kingdom is London.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'usage': {'acceptance_rate': 3, 'completion_tokens': 9, 'completion_tokens_after_first_per_sec': 48.0612352469348, 'completion_tokens_after_first_per_sec_first_ten': 59.127585939537475, 'completion_tokens_per_sec': 6.675974908458708, 'end_time': 1731021308.8695927, 'is_last_response': True, 'prompt_tokens': 42, 'start_time': 1731021307.5072339, 'time_to_first_token': 1.1959044933319092, 'total_latency': 1.348117709159851, 'total_tokens': 51, 'total_tokens_per_sec': 37.83052448126601}, 'model_name': 'llama3-405b', 'system_fingerprint': 'fastcoe', 'created': 1731021307}, id='b78c8432-60e0-4fa1-9913-604cb02c98e6')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.batch([\"tell me a joke\",\"which is the capital of UK?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='A man walked into a library and asked the librarian, \"Do you have any books on Pavlov\\'s dogs and Schrödinger\\'s cat?\"\\n\\nThe librarian replied, \"It rings a bell, but I\\'m not sure if it\\'s here or not.\"', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'usage': {'acceptance_rate': 4.25, 'completion_tokens': 53, 'completion_tokens_after_first_per_sec': 115.66867258063917, 'completion_tokens_after_first_per_sec_first_ten': 87.3483636078539, 'completion_tokens_per_sec': 32.619141509141876, 'end_time': 1731021312.8972187, 'is_last_response': True, 'prompt_tokens': 39, 'start_time': 1731021311.2724059, 'time_to_first_token': 1.175252914428711, 'total_latency': 1.6248128414154053, 'total_tokens': 92, 'total_tokens_per_sec': 56.621906015868916}, 'model_name': 'llama3-405b', 'system_fingerprint': 'fastcoe', 'created': 1731021311}, id='bea26b3b-9cce-4954-8ed0-9fe97912ca81'),\n",
       " AIMessage(content='The capital of the United Kingdom is London.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'usage': {'acceptance_rate': 7.5, 'completion_tokens': 9, 'completion_tokens_after_first_per_sec': 50.94757875333848, 'completion_tokens_after_first_per_sec_first_ten': 154.14417107268335, 'completion_tokens_per_sec': 7.295484394252751, 'end_time': 1731021312.604683, 'is_last_response': True, 'prompt_tokens': 42, 'start_time': 1731021311.2724059, 'time_to_first_token': 1.175252914428711, 'total_latency': 1.2336398124694825, 'total_tokens': 51, 'total_tokens_per_sec': 41.34107823409892}, 'model_name': 'llama3-405b', 'system_fingerprint': 'fastcoe', 'created': 1731021311}, id='bea26b3b-9cce-4954-8ed0-9fe97912ca81')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_responses = llm.abatch([\"tell me a joke\",\"which is the capital of UK?\"])\n",
    "await(future_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatSambaNovaCloud(\n",
    "    model= \"Meta-Llama-3.1-70B-Instruct\",\n",
    "    streaming=True,\n",
    "    max_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    top_k=1,\n",
    "    top_p=0.01,\n",
    "    stream_options={'include_usage':True}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A man walked into a \n",
      "library and asked the \n",
      "librarian, \"Do you have any books \n",
      "on Pavlov's dogs \n",
      "and Schrödinger's cat?\"\n",
      "\n",
      "\n",
      "The librarian \n",
      "\n",
      "replied, \"It rings a bell, \n",
      "but I'm not sure \n",
      "if it's here \n",
      "or not.\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(\"tell me a joke\"):\n",
    "    print(chunk.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Yer lookin' \n",
      "fer a joke, eh? \n",
      "Alright then, matey! \n",
      "Here be \n",
      "one fer ye:\n",
      "\n",
      "Why did \n",
      "the pirate quit \n",
      "his job?\n",
      "\n",
      "(pause fer \n",
      "dramatic effect)\n",
      "\n",
      "Because \n",
      "he was sick o' all \n",
      "the arrrr-guments!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Yarrr, hope that made ye \n",
      "laugh, \n",
      "me hearty!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant with pirate accent\"),\n",
    "    HumanMessage(content=\"tell me a joke\")\n",
    "    ]\n",
    "for chunk in llm.stream(messages):\n",
    "    print(chunk.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A man walked into a \n",
      "library and asked the \n",
      "librarian, \"Do you have any books \n",
      "on Pavlov's dogs \n",
      "and Schrödinger's cat?\"\n",
      "\n",
      "\n",
      "The librarian \n",
      "replied, \"It rings a bell, \n",
      "but I'm not sure \n",
      "if it's here \n",
      "or not.\"\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async for chunk in llm.astream(\"tell me a joke\"):\n",
    "    print(chunk.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fucntion calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from datetime import datetime\n",
    "\n",
    "@tool()\n",
    "def get_time(kind: str = 'both') -> str:\n",
    "    \"\"\"Returns current date, current time or both.\n",
    "\n",
    "    Args:\n",
    "        kind: date, time or both\n",
    "    \"\"\"\n",
    "    if kind == 'date':\n",
    "        date = datetime.now().strftime('%d/%m/%Y')\n",
    "        return f'Current date: {date}'\n",
    "    elif kind == 'time':\n",
    "        time = datetime.now().strftime('%H:%M:%S')\n",
    "        return f'Current time: {time}'\n",
    "    else:\n",
    "        date = datetime.now().strftime('%d/%m/%Y')\n",
    "        time = datetime.now().strftime('%H:%M:%S')\n",
    "        return f'Current date: {date}, Current time: {time}'\n",
    "    \n",
    "@tool()  \n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "tools=[get_time, add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatSambaNovaCloud(\n",
    "    model=\"Meta-Llama-3.1-70B-Instruct\",\n",
    "    max_tokens=1024,\n",
    "    temperature=0.3,\n",
    "    top_k=1,\n",
    "    top_p=0.01,\n",
    ")\n",
    "tool_llm=llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'get_time',\n",
       "  'args': {'kind': 'time'},\n",
       "  'id': 'call_3af9651b48894d74ae',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=tool_llm.invoke(\"what time is it?\")\n",
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sambastudio Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = SambaStudioEmbeddings(batch_size=1, model_kwargs={'select_expert': 'e5-mistral-7b-instruct'})\n",
    "embedding.embed_documents(['tell me a 50 word tale', 'tell me a joke'])\n",
    "embedding.embed_query('tell me a 50 word tale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='tell me a 50 word tale'),\n",
       " Document(page_content='tell me a joke'),\n",
       " Document(page_content='give me 3 party activities'),\n",
       " Document(page_content='give me three healty dishes')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "docs = [\n",
    "    'tell me a 50 word tale',\n",
    "    'tell me a joke',\n",
    "    'when was America discoverd?',\n",
    "    'how to build an engine?',\n",
    "    'give me 3 party activities',\n",
    "    'give me three healty dishes',\n",
    "]\n",
    "docs = [Document(doc) for doc in docs]\n",
    "\n",
    "query = 'prompt for generating something fun'\n",
    "\n",
    "vectordb = Chroma.from_documents(docs, embedding)\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
